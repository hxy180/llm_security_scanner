# 开发语言无关的 LLM 驱动代码漏洞扫描器

**重要提示：** 本```研究```是利用 ChatGPT Deep Research 生成的，旨在帮助自动化研究和创意构思。

## 简介

像 OpenAI 的 GPT 系列这样的大型语言模型 (LLM) 已经展现出惊人的代码理解和生成能力，为自动化安全代码审查开辟了新的可能性。传统的静态分析工具依赖于预定义的规则，往往面临误报率高或语言特定的局限性。相比之下，基于 LLM 的漏洞扫描器可以利用模型对跨语言常见弱点模式的知识。例如，一项使用 GPT-3 扫描易受攻击代码库的实验发现了 213 个潜在问题，而顶级商业扫描器仅发现了 99 个。本报告探讨了使用 LLM（如 GPT-4 Turbo）构建语言无关代码漏洞扫描器的最佳实践，该扫描器可以：遍历仓库目录，识别每个源文件的编程语言，分析代码中的安全弱点（例如注入、不安全的反序列化、使用易受攻击的 API），并报告潜在漏洞的文件名和行号。我们还讨论了有效的模型选择、用于可靠检测的提示词工程技术、多语言支持、LLM 扫描的实践案例，以及如何将扫描器集成到 GitHub Actions CI/CD 工作流中以实现自动问题报告。

## 选择用于漏洞扫描的 OpenAI 模型

选择合适的 LLM 对于有效的代码漏洞检测至关重要。GPT-4（及其变体）目前是 OpenAI 产品线中处理复杂推理和代码理解能力最强的模型。研究表明，GPT-4 在识别安全缺陷方面显著优于 GPT-3.5。在对 C++、Java 和 Python 代码的一项评估中，改进后的 GPT-4 模型通过最佳提示词实现了约 0.90 的 F1 漏洞检测分数，远高于 GPT-3.5 Turbo。同样，一个结合了静态分析与 GPT-4 的研究原型 (IRIS) 在真实世界的 Java 项目中检测到了 120 个已知漏洞中的 55 个，而仅使用 CodeQL 仅检测到 27 个。这些结果表明，GPT-4 更深层次的理解能力可以捕捉更多问题，包括纯基于规则的扫描器可能遗漏的逻辑漏洞。

在实现扫描器时，GPT-4 Turbo（8k 上下文）或 32k 上下文版本是理想的选择，因为它们可以在单个提示词中处理较大的代码文件。如果需要，更大的上下文窗口允许一次扫描数百行或多个文件。GPT-3.5 Turbo（4k 上下文）是一种成本较低的替代方案，但它可能会遗漏更细微的问题，并且总体检测率较低。使用 GPT-3 (text-davinci-003) 的早期实验证明了 LLM 扫描的可行性，但也突出了上下文大小和偶尔疏忽等局限性。例如，由于上下文限制，GPT-3 必须单独分析文件，这意味着可能会遗漏需要整个仓库推理的跨文件漏洞。像 GPT-4 这样的较新模型通过更多的 Token 和更高的准确性缓解了其中一些问题。总之，鉴于其卓越的代码理解和漏洞检测性能，GPT-4 Turbo 是语言无关扫描器的推荐模型，而 GPT-3.5 可用于初步扫描或预算敏感的场景（承认召回率/精确度会有所下降）。

## 用于可靠漏洞检测的提示词工程

设计有效的提示词对于引导 LLM 生成准确且全面的漏洞报告至关重要。与人工代码审查不同，AI 需要明确的指令来了解要查找的内容以及如何格式化其发现。提示词工程的最佳实践包括：

- **角色和任务规范**：明确指示模型它正在充当安全审计员。例如，系统消息可以说“你是一位审查代码漏洞的网络安全专家”。这会引导 LLM 采用以安全为中心的心态。

- **请求具体细节和上下文**：提供代码（或相关摘录）并要求模型识别任何安全漏洞，最好包含缺陷类型、解释及其发生位置。如果已知，提及编程语言或框架会有所帮助，以便 LLM 可以应用特定于语言的安全知识。例如：“分析以下 Python 代码是否存在安全漏洞，如注入、不安全的反序列化、使用不安全函数等。提供发现的任何漏洞的列表，包括类别和解释，并引用受影响的行号。”在提示词中包含一份简短的漏洞类别列表可以提醒模型检查这些问题，但提示词也应鼓励“任何其他潜在的安全问题”，以免遗漏不太常见的缺陷。

- **思维链和分步提示**：鼓励 LLM 逐步推理可以提高检测的可靠性。研究表明，分步提示比直接查询产生更好的结果。例如，提示词可能会说：“逐步思考代码，并考虑攻击者可能如何利用它。首先，识别潜在的危险操作或输入，然后确定是否到位了适当的验证或保障措施。”这可以引导 GPT-4 系统地评估代码的每个部分是否存在弱点，而不是得出一个快速（且可能肤浅）的答案。一项研究发现，分步提示导致 GPT-4 变体的检测 F1 分数提升最大。

- **输出格式化**：要求模型以结构化格式输出发现结果，以便于解析或包含在报告中。例如，您可以请求一个项目符号列表，其中每个项目包括漏洞类型、描述和位置。如果提供的代码片段带有行号（下一节将详细介绍），模型可以引用这些行号。例如：“1. SQL 注入 – 未经清理的用户输入查询被连接到 SQL 查询中（第 42 行）。”结构化输出使得更容易汇总跨文件的最终漏洞列表。

- **少样本示例**：在某些情况下，在提示词中提供一个小演示会有所帮助。您可以在要求模型分析实际目标代码之前，包含一个简短的代码片段和示例分析（作为指南）。这是一种少样本提示形式，可以校准模型的响应。但是，请注意添加示例会消耗部分 Token 上下文；凭借 GPT-4 的高级能力，明确的指令通常就足够了。

根据结果迭代和完善提示词非常重要。如果您发现模型遗漏了某些类型的问题，可以调整措辞或在提示词中添加额外的提示。相反，如果模型产生不存在问题的幻觉，您可能需要限制提示词以专注于事实代码分析。研究指出，提示词措辞会影响误报率。在实践中，定义任务和预期输出格式的简洁但直接的提示词往往对代码扫描效果很好。下面是一个单文件分析的示例提示词：

```
你是一位安全专家。分析以下 JavaScript 代码是否存在任何安全漏洞（例如，XSS、SQL 注入、命令注入、不安全的反序列化、使用易受攻击的库等）。提供每个潜在漏洞的列表，并简要解释和说明其发生的行号。如果代码是安全的，请说明未发现问题。

```javascript
// File: utils/db.js
${code_contents_with_line_numbers}
```

在上面的提示词中，请注意代码标有文件路径并封装在带有语法提示 (JavaScript) 的代码块中 – 这有助于 LLM 识别语言。在代码中包含行号（可以通过扫描脚本预先生成）允许模型引用特定位置。模型对该提示词的回答可能如下所示：

- *SQL 注入*：在函数 `runQuery` 的 **第 27 行**，用户输入 `query` 未经清理直接连接到 SQL 字符串中。这可能允许攻击者注入 SQL 命令。
- *无输出编码*：函数 `renderComments` 在 **第 52 行** 将用户提供的评论文本插入 HTML 而未进行编码，使应用程序容易受到 XSS 攻击。

每个发现都包括漏洞的性质及其在文件中的发生位置。良好的提示词设计将鼓励 LLM 以这种方式做到彻底和清晰。

## 语言检测和多语言代码解析

为了支持扫描**多语言仓库**，扫描器需要识别每个源文件的语言，并可能相应地调整分析。最简单的方法是使用文件的扩展名和已知约定。大多数编程语言都可以从其文件扩展名推断出来（`.py` 代表 Python，`.js` 代表 JavaScript，`.java` 代表 Java 等），这在常见情况下既快速又可靠。例如，以 `.py` 结尾的文件可以自信地归类为 Python。在扩展名模棱两可或缺失的情况下（例如没有扩展名的脚本或可能是 C 或 C++ 的 `.h` 头文件），进行一些内容分析是有用的。检查 **shebang 行**（例如 `#!/usr/bin/env python3`）可以揭示没有扩展名的脚本的语言。对于 `.h` 头文件或其他共享扩展名，检查内容中的关键字（如 `#include` 与 `import`，或类定义语法）可以消除 C 与 C++ 与 Objective-C 的歧义。GitHub 的 Linguist 等工具或库（例如 **LanguageSniffer** 或 **guesslang**）实现了这些启发式方法 – 例如，LanguageSniffer 执行“深度内容检查”以处理边缘情况，例如通过查找特定于语言的关键字来区分 C/C++ 头文件。在我们的扫描器中，遍历仓库目录后，我们可以使用扩展名到语言的映射，并辅以基于内容的检查来为每个文件标记语言。然后可以将此语言信息传递到 LLM 提示词中（如上面的示例提示词所示），以帮助模型应用相关的漏洞知识（例如，针对 PHP 文件中的 SQL 注入发出警报，或针对 Shell 脚本中的命令注入发出警报等）。

多语言支持的另一个方面是在将代码发送到 LLM 之前处理特定于语言的**代码解析或结构化**。通常，LLM 不需要抽象语法树或正式解析 – 它们可以直接处理原始代码文本。但是，准备代码片段可以改善结果。明智的做法是**删除或跳过大型自动生成的文件或依赖项**（如 `node_modules/`、压缩的 JS 或编译产物） – 这些不太可能包含您自己代码中的漏洞，并且会浪费 Token 空间。专注于第一方源文件。如果文件非常大（接近或超过模型的上下文限制），您有几个选择：

- **分块**：将文件拆分为逻辑块（例如，按函数或类，或约 200-300 行的块）并使用 LLM 单独分析每个块。这确保每个提示词都保持在 Token 限制内。然后扫描器可以汇总该文件的发现结果。分块时，如果可能，请在每个块中包含上下文（例如，重叠几行或在提示词中提及“这是文件的第 X 部分”以提供连续性）。请记住，分块可能会导致模型遗漏跨越块边界的漏洞（例如，在一个块中定义的受污染变量在另一个块中使用）。这是上下文大小所必需的权衡。

- **跨文件上下文摘要**：对于涉及多个文件的漏洞（如在一个文件中滥用 API 并在另一个文件中配置不安全），基本扫描器可能会遗漏这些，因为它独立分析文件。更高级的方法是使用 LLM 总结每个文件中的重要行为（或识别入口点），然后对这些摘要进行二次分析以发现多文件问题。这是一个复杂的增强功能，有些实验性。在实践中，许多漏洞可以在单个文件的上下文中找到，特别是注入或函数滥用，基于 GPT 的扫描器在逐文件分析方面取得的成功证明了这一点。

在解析过程中，如前所述，**用行号注释代码**也很有帮助。一种简单的方法是在每一行前面加上其编号作为注释或在文本中（模型将其视为代码上下文的一部分）。例如：`1: def vulnerable_function(user_input):` 等等。然后模型可以在其输出中提及“第 1 行”。如果不这样做，扫描器仍然可以尝试通过在文件中搜索报告的代码片段将模型的输出映射回行号 – 但预先提供行号更直接。

最后，确保扫描器识别**多种编程语言**并为每种语言使用适当的提示词。您可以维护一个特定于语言的提示词调整或漏洞关注点字典。例如，对于 C/C++ 代码，您可能会提醒模型检查内存安全问题（缓冲区溢出、释放后使用），而对于 Python，您会强调不安全的反序列化或使用 `exec/eval`，对于像 JavaScript/TS 这样的 Web 语言，请考虑 XSS、CSRF 等。核心扫描逻辑保持不变；只是可以根据每种语言调整上下文提示，以获得最佳的漏洞覆盖率。

## 基于 LLM 的漏洞扫描示例

最近的实验和项目证明了 LLM 驱动的代码扫描的可行性，并提供了有关有效技术的见解：

- **GPT-3 实验性扫描器 (Chris Koch, 2023)**：在一项公开实验中，一位安全研究人员使用 OpenAI 的 GPT-3 (text-davinci-003) 扫描了一个故意易受攻击的代码库。结果出奇地强劲 – GPT-3 识别了 129 个文件中的 **200 多个漏洞**，而商业静态分析工具仅发现了 99 个。GPT-3 的发现包括格式字符串利用、日志注入和缓冲区管理不善等问题，人工审查表明误报率很低（60 个抽样报告中只有 4 个是误报）。这展示了 LLM 跨不同语言（测试仓库包含 C、C#、Python 等，每个都有已知缺陷）捕捉安全问题的原始能力。然而，该实验也指出了局限性：由于 4000 Token 的上下文限制，扫描器必须单独分析每个文件，这意味着可能会遗漏跨越多个文件或需要全局上下文的漏洞。确实，GPT-3 有时可以推断出跨文件问题，如果它们涉及常见的库（可能是因为模型“知道”这些库的典型用法），但这并非万无一失。该实验使用了相对简单的提示词，仍然实现了良好的覆盖率，表明即使没有复杂的提示词调整，LLM 也对常见漏洞具有很强的基准知识。（值得注意的是，GPT-4 更先进，在这种情况下大概会表现得更好。）另一方面，对该实验的外部分析指出，基于 GPT 的分析可能会产生**误报**，经验不足的用户可能会误以为正确。例如，GPT-3 标记了一个 C 代码片段中的“未验证的用户输入可能导致缓冲区溢出”，而实际上该问题并非真正的缓冲区溢出，这表明模型的解释可能具有误导性。

- **IRIS – 神经符号分析 (2024)**：学术研究人员开发了 IRIS，这是一个结合了 GPT-4 和静态分析的系统，用于**全仓库**漏洞推理。在他们的方法中，GPT-4 被提示推断*污点流规范*并执行传统静态分析器缺乏的上下文推理。例如，LLM 会猜测哪些函数是不可信数据的来源、汇点或清理器，然后静态分析会对照代码检查这些猜测。这种 AI 直觉与形式化分析的结合证明是有效的 – IRIS 在 Java 基准测试中检测到的漏洞是 CodeQL 的两倍多，甚至发现了一些新的 0-day 问题。对我们的启示是，LLM 可以通过提供通常需要人类专业知识才能编码的见解（例如可能的数据流或安全规则）来增强静态分析。虽然 IRIS 是一个复杂的解决方案，但它表明即使是更简单的 LLM 扫描器也可以从一些静态分析帮助中受益 – 例如，可以使用轻量级静态工具来识别可疑模式或输入，然后让 LLM 详细分析这些内容。相反，LLM 可以标记静态分析遗漏的内容，例如不安全的逻辑，并且可以对这些内容进行复查。

- **提示词工程研究**：多项研究集中在如何提示 LLM 以获得更好的漏洞检测。*Zhang 等人* (2023) 探索了针对 Java 和 C/C++ 代码的**提示增强型漏洞检测**，发现查询的措辞方式会显著影响 ChatGPT 的检测能力。*David 等人* (2023) 优化了用于智能合约安全分析的提示词，并指出虽然 GPT-4 可以捕捉许多问题，但在他们的测试中也有**很高的误报率**，表明提示词需要仔细校准。这些研究中的一个有效策略是在提示词中将任务分解为子任务（例如，首先列出所有输入和敏感操作，然后分析每个操作是否存在潜在滥用） – 本质上是引导模型的推理。另一个策略是使用提示词集合：例如，**GPTLens**（一个智能合约框架）运行多个具有不同提示词的审计代理以生成各种漏洞报告，然后使用批评代理（另一个 GPT-4 实例）合并和排名发现结果。这种两阶段方法（生成 + 判别）减少了随机错误，并通过让模型“复查”自己的工作提高了准确性。虽然每个文件运行多个 LLM 调用对于 CI 工作流来说可能太慢或太昂贵，但这些想法可以激发未来的增强功能 – 例如，使用一个提示词运行快速扫描，然后针对同一代码重新提示模型以验证或详细说明发现的问题，从而过滤掉虚假结果。

*高级基于 LLM 的漏洞扫描框架示例。**GPTLens** 使用多个 GPT-4 审计员生成关于智能合约的独立安全报告，然后批评代理合并和排名结果（减少错误）。**LLM4Vuln** 评估不同的提示方案（原始代码与带提示的代码等）以提高模型对漏洞的推理能力。**GPTScan** 将基于 GPT 的模式匹配与静态分析确认相结合，以检测智能合约逻辑缺陷。这些方法说明了提示词设计和混合技术如何在特定领域提高准确性。*
